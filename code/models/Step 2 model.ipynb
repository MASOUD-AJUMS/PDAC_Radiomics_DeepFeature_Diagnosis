{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using feature selection method: SelectKBest_MI\n",
      "Training LDA_Classifier with SelectKBest_MI...\n",
      "Training XGBoost with SelectKBest_MI...\n",
      "Training LightGBM with SelectKBest_MI...\n",
      "[LightGBM] [Info] Number of positive: 3063, number of negative: 3063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 6126, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training CatBoost with SelectKBest_MI...\n",
      "Training GradientBoosting with SelectKBest_MI...\n",
      "Training AdaBoost with SelectKBest_MI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with SelectKBest_MI...\n",
      "\n",
      "Using feature selection method: SelectKBest_ANOVA\n",
      "Training LDA_Classifier with SelectKBest_ANOVA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 10  22  77 100 115 135 174 195 234 241] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost with SelectKBest_ANOVA...\n",
      "Training LightGBM with SelectKBest_ANOVA...\n",
      "[LightGBM] [Info] Number of positive: 3063, number of negative: 3063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 6126, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training CatBoost with SelectKBest_ANOVA...\n",
      "Training GradientBoosting with SelectKBest_ANOVA...\n",
      "Training AdaBoost with SelectKBest_ANOVA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with SelectKBest_ANOVA...\n",
      "\n",
      "Using feature selection method: VarianceThreshold\n",
      "Training LDA_Classifier with VarianceThreshold...\n",
      "Training XGBoost with VarianceThreshold...\n",
      "Training LightGBM with VarianceThreshold...\n",
      "[LightGBM] [Info] Number of positive: 3063, number of negative: 3063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 60162\n",
      "[LightGBM] [Info] Number of data points in the train set: 6126, number of used features: 244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training CatBoost with VarianceThreshold...\n",
      "Training GradientBoosting with VarianceThreshold...\n",
      "Training AdaBoost with VarianceThreshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with VarianceThreshold...\n",
      "\n",
      "Using feature selection method: SelectFromModel_RF\n",
      "Training LDA_Classifier with SelectFromModel_RF...\n",
      "Training XGBoost with SelectFromModel_RF...\n",
      "Training LightGBM with SelectFromModel_RF...\n",
      "[LightGBM] [Info] Number of positive: 3063, number of negative: 3063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 6126, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training CatBoost with SelectFromModel_RF...\n",
      "Training GradientBoosting with SelectFromModel_RF...\n",
      "Training AdaBoost with SelectFromModel_RF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with SelectFromModel_RF...\n",
      "\n",
      "Using feature selection method: SelectFromModel_ET\n",
      "Training LDA_Classifier with SelectFromModel_ET...\n",
      "Training XGBoost with SelectFromModel_ET...\n",
      "Training LightGBM with SelectFromModel_ET...\n",
      "[LightGBM] [Info] Number of positive: 3063, number of negative: 3063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 6126, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training CatBoost with SelectFromModel_ET...\n",
      "Training GradientBoosting with SelectFromModel_ET...\n",
      "Training AdaBoost with SelectFromModel_ET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with SelectFromModel_ET...\n",
      "\n",
      "All models trained and results saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, precision_recall_fscore_support, accuracy_score\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, mutual_info_classif, f_classif,\n",
    "    SelectFromModel, VarianceThreshold\n",
    ")\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Function to add Hard Voting and Stacking classifiers\n",
    "def add_hard_voting_and_stacking(X_train, y_train, X_test):\n",
    "    clf1 = RandomForestClassifier(random_state=42)\n",
    "    clf2 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf3 = SVC(probability=True, random_state=42)\n",
    "    classifiers = [('rf', clf1), ('lr', clf2), ('svc', clf3)]\n",
    "\n",
    "    # Hard Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    voting_preds = voting_clf.predict(X_test)\n",
    "\n",
    "    # Stacking Classifier\n",
    "    meta_clf = LogisticRegression(random_state=42)\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=classifiers, final_estimator=meta_clf, cv=5\n",
    "    )\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    stacking_preds = stacking_clf.predict(X_test)\n",
    "\n",
    "    return voting_clf, voting_preds, stacking_clf, stacking_preds\n",
    "\n",
    "# Function to save results and draw ROC plot\n",
    "def save_results_and_draw_roc(clf, X_test, y_test, preds, file_name, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Compute ROC AUC if possible\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    except Exception as e:\n",
    "        roc_auc = np.nan\n",
    "        print(f\"Could not compute ROC AUC for {file_name}: {e}\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    # Save classification report with added AUC and Accuracy rows\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "    # Add ROC AUC row (placing the value in precision, recall, and f1-score columns)\n",
    "    report[\"ROC_AUC\"] = {\"precision\": roc_auc, \"recall\": roc_auc, \"f1-score\": roc_auc, \"support\": \"\"}\n",
    "    # Add Overall Accuracy row\n",
    "    report[\"Overall_Accuracy\"] = {\"precision\": accuracy, \"recall\": accuracy, \"f1-score\": accuracy, \"support\": \"\"}\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_excel(\n",
    "        os.path.join(output_dir, f'{file_name}_classification_report.xlsx'),\n",
    "        index=True\n",
    "    )\n",
    "\n",
    "    # Save confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, preds)\n",
    "    df_conf_matrix = pd.DataFrame(conf_matrix)\n",
    "    df_conf_matrix.to_excel(\n",
    "        os.path.join(output_dir, f'{file_name}_confusion_matrix.xlsx'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Try to compute ROC and plot if possible\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        plt.figure()\n",
    "        plt.plot(\n",
    "            fpr, tpr, color='blue', lw=2,\n",
    "            label='ROC curve (area = %0.2f)' % roc_auc\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{file_name} ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, f'{file_name}_roc_curve.png')\n",
    "        )\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate ROC curve for {file_name}: {e}\")\n",
    "\n",
    "# Function to generate charts\n",
    "def generate_charts(df_results, output_dir):\n",
    "    metrics = ['Recall', 'Precision', 'F1-Score', 'AUC', 'Accuracy']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pivot_table = df_results.pivot(\n",
    "            index='Feature_Selection', columns='Model', values=metric\n",
    "        )\n",
    "        pivot_table.plot(kind='bar', ax=plt.gca())\n",
    "        plt.title(\n",
    "            f'{metric} Scores for Different Models and Feature Selection Methods'\n",
    "        )\n",
    "        plt.xlabel('Feature Selection Method')\n",
    "        plt.ylabel(metric)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(\n",
    "            title='Model', bbox_to_anchor=(1.05, 1), loc='upper left'\n",
    "        )\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, f'{metric}_comparison_chart.png')\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "# Function to process file with multiple models and feature selection methods\n",
    "def process_file(file_path, output_dir):\n",
    "    # Load data\n",
    "    data = pd.read_excel(file_path)\n",
    "    data_cleaned = data.drop(columns=['File Name'], errors='ignore')\n",
    "    X = pd.get_dummies(data_cleaned.drop(columns=['Cancer Status']), drop_first=True)\n",
    "    y = data_cleaned['Cancer Status']\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(\n",
    "        X_train, y_train\n",
    "    )\n",
    "\n",
    "    # Feature selection methods\n",
    "    feature_selection_methods = {\n",
    "        'SelectKBest_MI': SelectKBest(score_func=mutual_info_classif, k=20),\n",
    "        'SelectKBest_ANOVA': SelectKBest(score_func=f_classif, k=20),\n",
    "        'VarianceThreshold': VarianceThreshold(threshold=0.1),\n",
    "        'SelectFromModel_RF': SelectFromModel(\n",
    "            RandomForestClassifier(random_state=42), max_features=20\n",
    "        ),\n",
    "        'SelectFromModel_ET': SelectFromModel(\n",
    "            ExtraTreesClassifier(random_state=42), max_features=20\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models = {\n",
    "        \"LDA_Classifier\": LinearDiscriminantAnalysis(),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            scale_pos_weight=5, random_state=42, eval_metric='logloss'\n",
    "        ),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"MLP\": MLPClassifier(\n",
    "            hidden_layer_sizes=(100,), max_iter=1000, random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Dictionary to store results (including AUC and Accuracy)\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Feature_Selection': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'AUC': [],\n",
    "        'Accuracy': []\n",
    "    }\n",
    "\n",
    "    # Loop over feature selection methods\n",
    "    for fs_name, selector in feature_selection_methods.items():\n",
    "        print(f\"\\nUsing feature selection method: {fs_name}\")\n",
    "\n",
    "        # Fit the selector on training data\n",
    "        selector.fit(X_train_balanced, y_train_balanced)\n",
    "        X_train_selected = selector.transform(X_train_balanced)\n",
    "        X_test_selected = selector.transform(X_test)\n",
    "\n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Training {model_name} with {fs_name}...\")\n",
    "\n",
    "            model.fit(X_train_selected, y_train_balanced)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "\n",
    "            # Calculate metrics using macro average\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "                y_test, y_pred, average='macro', zero_division=0\n",
    "            )\n",
    "\n",
    "            # Compute ROC AUC; use try/except in case predict_proba isn't available\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, model.predict_proba(X_test_selected)[:, 1])\n",
    "            except Exception as e:\n",
    "                auc = np.nan\n",
    "                print(f\"Could not compute AUC for {model_name} with {fs_name}: {e}\")\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            # Store the macro average metrics along with AUC and Accuracy\n",
    "            results['Model'].append(model_name)\n",
    "            results['Feature_Selection'].append(fs_name)\n",
    "            results['Precision'].append(precision)\n",
    "            results['Recall'].append(recall)\n",
    "            results['F1-Score'].append(f1_score)\n",
    "            results['AUC'].append(auc)\n",
    "            results['Accuracy'].append(acc)\n",
    "\n",
    "            # Save individual results\n",
    "            model_fs_name = f\"{model_name}_{fs_name}\"\n",
    "            save_results_and_draw_roc(\n",
    "                model, X_test_selected, y_test, y_pred,\n",
    "                model_fs_name, output_dir\n",
    "            )\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_excel(\n",
    "        os.path.join(output_dir, 'combined_results.xlsx'), index=False\n",
    "    )\n",
    "\n",
    "    # Generate comparison charts\n",
    "    generate_charts(df_results, output_dir)\n",
    "\n",
    "    print(\"\\nAll models trained and results saved!\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Set paths - UPDATE THESE TO MATCH YOUR DRIVE STRUCTURE\n",
    "    input_path = r\"c:\\Users\\alire\\Downloads\\combinedoutput.xlsx\"  # Update with your file path\n",
    "    output_dir = r\"E:\\step 2 deep feature_with ROC and AUC(0.3)\"  # Update with desired output directory\n",
    "\n",
    "    # Create output directory if not exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    process_file(input_path, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
