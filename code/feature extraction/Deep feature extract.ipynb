{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab_0almWhg76"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aH6R0bsNHPvN"
   },
   "outputs": [],
   "source": [
    "!pip install nibabel numpy pandas tensorflow scikit-learn scipy openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h8LdLO-MuUM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# 1) Define the autoencoder\n",
    "def create_autoencoder(input_shape):\n",
    "    input_img = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "    x = layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "\n",
    "    # --- Bottleneck ---\n",
    "    shape_before_flattening = x.shape[1:]\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    bottleneck = layers.Dense(512, activation='tanh')(x)\n",
    "    bottleneck = layers.Dense(256, activation='tanh')(bottleneck)\n",
    "\n",
    "    # --- Decoder ---\n",
    "    x = layers.Dense(np.prod(shape_before_flattening), activation='relu')(bottleneck)\n",
    "    x = layers.Reshape(shape_before_flattening)(x)\n",
    "    x = layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling3D((2, 2, 2))(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling3D((2, 2, 2))(x)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling3D((2, 2, 2))(x)\n",
    "    decoded = layers.Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    encoder = models.Model(input_img, bottleneck)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# 2) Load and resize NIfTI files (with a mask option to avoid label corruption)\n",
    "def load_and_resize_nifti(file_path, target_shape, is_mask=False):\n",
    "    img = nib.load(file_path)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    zoom_factors = [float(t) / float(s) for t, s in zip(target_shape, data.shape)]\n",
    "\n",
    "    interpolation_order = 0 if is_mask else 1\n",
    "\n",
    "    resized_data = zoom(data, zoom_factors, order=interpolation_order)\n",
    "    return resized_data\n",
    "\n",
    "# 3) Process dataset\n",
    "def process_dataset(image_dir, mask_dir, encoder, target_shape):\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        if file_name.endswith('.nii.gz'):\n",
    "            image_path = os.path.join(image_dir, file_name)\n",
    "            print(f\"Processing image: {file_name}\")\n",
    "\n",
    "            image_data = load_and_resize_nifti(image_path, target_shape, is_mask=False)\n",
    "\n",
    "            mask_file_name = next((m for m in os.listdir(mask_dir) if m.startswith(file_name[:6]) and m.endswith('.nii.gz')), None)\n",
    "            if mask_file_name:\n",
    "                mask_path = os.path.join(mask_dir, mask_file_name)\n",
    "                print(f\"Processing mask: {mask_file_name}\")\n",
    "                mask_data = load_and_resize_nifti(mask_path, target_shape, is_mask=True)\n",
    "\n",
    "                mask_data = np.round(mask_data).astype(int)\n",
    "\n",
    "                if image_data.shape != mask_data.shape:\n",
    "                    print(f\"Shape mismatch for {file_name}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                unique_labels = np.unique(mask_data)\n",
    "                unique_labels = unique_labels[(unique_labels >= 1) & (unique_labels <= 6)]\n",
    "\n",
    "                print(f\"Valid labels in mask {file_name}: {unique_labels}\")\n",
    "\n",
    "                if len(unique_labels) == 0:\n",
    "                    print(f\"No valid labels found in mask {file_name}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                patient_features = []\n",
    "                for label in unique_labels:\n",
    "                    binary_mask = (mask_data == label).astype(np.float32)\n",
    "\n",
    "                    masked_image = image_data * binary_mask\n",
    "\n",
    "                    # Ensure we process only the region covered by the mask\n",
    "                    non_zero_indices = np.where(binary_mask > 0)\n",
    "                    if non_zero_indices[0].size == 0:\n",
    "                        print(f\"No regions found for label {label} in {file_name}, skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    cropped_image = masked_image[np.min(non_zero_indices[0]):np.max(non_zero_indices[0])+1,\n",
    "                                                 np.min(non_zero_indices[1]):np.max(non_zero_indices[1])+1,\n",
    "                                                 np.min(non_zero_indices[2]):np.max(non_zero_indices[2])+1]\n",
    "\n",
    "                    # Resize cropped image to target_shape\n",
    "                    zoom_factors = [float(t) / float(s) for t, s in zip(target_shape, cropped_image.shape)]\n",
    "                    resized_cropped_image = zoom(cropped_image, zoom_factors, order=1)\n",
    "\n",
    "                    # Normalize resized image\n",
    "                    max_val = np.max(resized_cropped_image)\n",
    "                    if max_val > 0:\n",
    "                        normalized_image = (resized_cropped_image - np.mean(resized_cropped_image)) / (np.std(resized_cropped_image) + 1e-8)\n",
    "                    else:\n",
    "                        normalized_image = resized_cropped_image\n",
    "\n",
    "                    normalized_image = np.expand_dims(normalized_image, axis=-1)\n",
    "                    normalized_image = np.expand_dims(normalized_image, axis=0)\n",
    "\n",
    "                    try:\n",
    "                        deep_features = encoder.predict(normalized_image)\n",
    "                        patient_features.append((int(label), deep_features.flatten()))\n",
    "                        print(f\"Extracted features for {file_name}, label {label}.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting features for {file_name}, label {label}: {e}\")\n",
    "\n",
    "                if patient_features:\n",
    "                    results.append((file_name, patient_features))\n",
    "            else:\n",
    "                print(f\"Mask not found for image {file_name}, skipping.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# 4) Save results to Excel (هر لیبل یک ردیف مجزا)\n",
    "def save_to_excel(results, output_file):\n",
    "    if not results:\n",
    "        print(\"No results to save. The results list is empty.\")\n",
    "        return\n",
    "\n",
    "    data = []\n",
    "    num_features = len(results[0][1][0][1])\n",
    "\n",
    "    for file_name, patient_features in results:\n",
    "        for (label, feature_vec) in patient_features:\n",
    "            row = [file_name, label] + feature_vec.tolist()\n",
    "            data.append(row)\n",
    "\n",
    "    columns = ['File Name', 'Label'] + [f'Feature_{i+1}' for i in range(num_features)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Results successfully saved to {output_file}\")\n",
    "\n",
    "# 5) Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = \"/content/drive/MyDrive/PANAROMA/pancreatic classification/image & label/image_batch3\"\n",
    "    mask_dir = \"/content/drive/MyDrive/PANAROMA/pancreatic classification/image & label/label_batch3\"\n",
    "    output_file = \"/content/drive/MyDrive/1/deep_features01.xlsx\"\n",
    "\n",
    "    target_shape = (128, 128, 64)\n",
    "    input_shape = target_shape + (1,)\n",
    "\n",
    "    autoencoder, encoder = create_autoencoder(input_shape)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    results = process_dataset(image_dir, mask_dir, encoder, target_shape)\n",
    "\n",
    "    save_to_excel(results, output_file)\n",
    "\n",
    "    print(f\"Deep features saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
