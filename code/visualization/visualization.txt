SHAP.ipynb
This Jupyter notebook focuses on processing Excel data for machine learning tasks, particularly using SHAP (SHapley Additive exPlanations) for model interpretation. It begins by loading the dataset, checking label distributions, and cleaning the data by removing irrelevant columns. The notebook employs techniques such as SMOTE for handling class imbalance, feature selection using mutual information, and training a Random Forest Classifier with specified class weights. After making predictions, it evaluates the model's performance through confusion matrices, ROC curves, and classification reports. Additionally, it visualizes the results using t-SNE plots and SHAP summary plots to interpret feature importance effectively.

t-SNE.ipynb
This notebook is dedicated to visualizing high-dimensional data using t-SNE (t-distributed Stochastic Neighbor Embedding). It begins by loading an Excel file containing labeled data, ensuring that the necessary 'label' column is present. The notebook separates the labels from the features and applies t-SNE to reduce the feature dimensions to two for easier visualization. It creates a DataFrame with the t-SNE results and labels, then employs seaborn to plot density contours and scatter points for each label group. The visualizations help in understanding the distribution and clustering of different classes within the dataset, enhancing interpretability.

Heat map code.ipynb
This Jupyter notebook generates heatmaps to visualize performance metrics from various machine learning models stored in Excel files. It begins by loading a specified Excel file and checking for required columns like "Model," "Feature Selection," and various performance metrics (Precision, Recall, F1-Score, Accuracy, AUC). The notebook combines model and feature selection into a single label for improved readability on the y-axis. It then prepares the data for visualization by converting metrics to percentages and utilizes seaborn to create a heatmap with annotations. The heatmap is styled for clarity, featuring bold titles and labels, and serves as a powerful tool for comparing model performance across different feature selection approaches.